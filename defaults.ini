
[DEFAULTS]

#name of the run
name = sketch2sound

# name of the project
project = s2s

# the batch size
batch_size = 16

# If `true`, attempts to resume training from latest checkpoint.
# In this case, each run must have unique config filename.
recover = false

# Save top K model checkpoints during training.
save_top_k = 2

# number of nodes to use for training
num_nodes = 1

# Multi-GPU strategy for PyTorch Lightning
strategy = "ddp_find_unused_parameters_true"

# Precision to use for training
precision = "16-mixed"

# number of CPU workers for the DataLoader
num_workers = 9

# the random seed
seed = 42

# Batches for gradient accumulation
accum_batches = 1

# Number of steps between checkpoints
checkpoint_every = 5000

# Maximum of steps for the training
max_steps = 40000

# Number of steps between validation runs
val_every = 5000

# trainer checkpoint file to restart training from
ckpt_path = ''

# model checkpoint file to start a new training run from
pretrained_ckpt_path = '/home/stud/dco/storage/dco/hf_cache/hub/models--stabilityai--stable-audio-open-1.0/snapshots/f21265c1e2710b3bd2386596943f0007f55f802e/model.safetensors'

# Checkpoint path for the pretransform model if needed
pretransform_ckpt_path = ''

# configuration model specifying model hyperparameters
model_config = '/home/stud/dco/Desktop/s2s-2/Controllable-Audio-Generation-from-Sonic-Imitations/stable_audio_tools/configs/model_configs/sketch2sound/s2s_impl.json'

# configuration for datasets
dataset_config = '/home/stud/dco/Desktop/s2s-2/Controllable-Audio-Generation-from-Sonic-Imitations/stable_audio_tools/configs/dataset_configs/s2s/train.json'

# configuration for validation datasets
val_dataset_config = '/home/stud/dco/Desktop/s2s-2/Controllable-Audio-Generation-from-Sonic-Imitations/stable_audio_tools/configs/dataset_configs/s2s/validation.json'

# directory to save the checkpoints in
save_dir = '/home/stud/dco/Desktop/s2s-2/Controllable-Audio-Generation-from-Sonic-Imitations/outdir'

# gradient_clip_val passed into PyTorch Lightning Trainer
gradient_clip_val = 0.0

# remove the weight norm from the pretransform model
remove_pretransform_weight_norm = ''

# Logger type to use
logger = 'wandb'
